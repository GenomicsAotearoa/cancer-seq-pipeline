// Trial pipeline
// Needs cutdapt, samtools, bwa, picard, java, varscan2, fastqc, bedtools. 
// Annovar script table_annovar.pl is a customised version of that provided with annovar, needs to replace the default version.
//      i.e. you need to install annovar and then replace table_annovar.pl with the copy found with this pipeline
// Report generation function to be added (soon)


//bwaIndex= baseDirectory + "data/references/hg19/ucsc.hg19.fasta"
//bwaIndex= "/blue/project/cancer-seq-pipeline/data/hg19/ucsc.hg19.fasta"
//bwaIndexDir= "$baseDirectory/data/hg19/"

cosmic= "/nesi/project/uoa02461/data/cosmicCodingMutations.vcf"
dbSnp150 = "/nesi/project/uoa02461/data/dbsnp150.vcf"
picardLoc = "/scale_wlg_nobackup/pan_migration/apps/easybuild/RHEL6.3/westmere/software/picard/2.1.0/picard.jar"
singularityBuilds = "baseDirectory/bin/"

qc = {	
    output.dir=$intermediateDirectory
    branch.sample=branch.name
    qcOutput=output.dir + "/fastqcResults"
    produce(sample + '_1.log', sample + '_2.log'){
        exec """fastqc -t 8 $input1 -o $qcOutput > $output1""", "qc"
        exec """fastqc -t 8 $input2 -o $qcOutput > $output2""", "qc"
    }
}

unzip = {

}


trim = {
    println"$intermediateDirectory"
    output.dir = intermediateDirectory
    println "$output.dir"
    branch.sample=branch.name
    println "Aligning: $sample"
    produce(sample + '_1.trim.gz','_2.trim.gz') {
        exec """singularity run --bind $dataDirectory --bind $intermediateDirectory $singularityBuilds/cutadapt.sif cutadapt --minimum-length 50 -a AGATCGGAAGAGC -q 30  -o $output1 -p $output2 $input1.fastq.gz $input2.fastq.gz""","trim"
        //exec """cutadapt --minimum-length 50 -a AGATCGGAAGAGC -q 30  -o $output1 -p $output2 $input1.fastq.gz $input2.fastq.gz""","trim"


    }
}

align = {
    output.dir=$intermediateDirectory
    requires type: 'the kind of sample being processed: test or control'
    branch.sample=branch.name
    
    unsortedBam = bindDir + sample + ".bam"    
    println "$unsortedBam"
    println "$input"
    produce(sample + '.' + type + '.bam') {
       exec """singularity run --bind $intermediateDirectory --bind $bwaIndexDir $singularityBuilds/bwa-0.7.17.simg bwa mem -t $threads -R "@RG\\tID:$sample\\tSM:$sample\\tLB:$sample\\tPL:ILLUMINA" $bwaIndex $input1 $input2 > $unsortedBam""","align"
       //exec """singularity run --bind $bindDir ../bin/samtools-1.9.simg samtools sort -@12 -O BAM -o $output.bam $unsortedBam""","align"
       exec """bwa mem -t $threads -R "@RG\\tID:$sample\\tSM:$sample\\tLB:$sample\\tPL:ILLUMINA" $bwaIndex $input1 $input2 | samtools sort -@12 -O BAM -o $output.bam ""","align"
       //exec """samtools sort -@12 -O BAM -o $output.bam $unsortedBam""","align"

    }

}


removeDuplicates = { 
    output.dir=$intermediateDirectory
    println "$input"
    exec """java -jar  $picardLoc MarkDuplicates I=$input.bam O=$output.bam M=$output._dup_metrics.txt REMOVE_DUPLICATES=TRUE CREATE_INDEX=TRUE""", "removeDuplicates"
}   

removeSuplementary = {
    output.dir=$intermediateDirectory
    println "$input"
    exec """samtools view -@ $threads -b -F 2048 $input.bam > $output.bam""","removeSupplementary"
    exec """samtools index $output.bam""","removeSupplementary"
}

sortBam = {   
    branch.sample=branch.name
    output.dir=$intermediateDirectory

    produce(sample + '.' + type + 'sorted.bam') {
        exec """singularity run --bind $bindDir ../bin/samtools-1.9.simg samtools sort -@12 -O BAM -o $output $input""","sortBam"
    }
}




markDuplicates = {
    output.dir=$intermediateDirectory
    // we only want to mark duplicates, not remove them. 
    exec "singularity run --bind $bindDir $singularityBuilds/picard-2.18.4.simg java -jar /opt/nesi/mahuika/picard/2.1.0/picard.jar MarkDuplicates I=$input.bam O=$output.bam M=$output._dup_metrics.txt TMP_DIR='/nesi/project/uoa02461/data/tmp' CREATE_INDEX=TRUE", "markDuplicates"
}

recalibrate = {
    output.dir="/nesi/nobackup/uoa02606/data/intermediateFiles"
    exec """ ../bin/gatk/gatk --java-options "-Xmx16G" BaseRecalibrator -R $bwaIndex -I $input.bam -O $output.table --known-sites /scale_wlg_persistent/filesets/project/uoa02606/data/gatk/dbsnp_138.hg19.vcf --known-sites /scale_wlg_persistent/filesets/project/uoa02606/data/gatk/Mills_and_1000G_gold_standard.indels.hg19.sites.vcf""", "recalibrate"
    exec """ ../bin/gatk/gatk --java-options "-Xmx16G" ApplyBQSR -R $bwaIndex -I $input.bam -bqsr $output.table -O $output.bam""", "recalibrate"

}

haplotypeCalling = {
    output.dir="/nesi/nobackup/uoa02606/data/intermediateFiles"
    exec """/scale_wlg_persistent/filesets/project/uoa02606/bin/gatk/gatk --java-options "-Xmx32G" HaplotypeCaller -R $bwaIndex -I $input.bam -O $output.vcf""" ,"haplotyping"
}



collectMetrics = {
    output.dir="/nesi/nobackup/uoa02461/data/intermediateFiles/metrics"
    requires type: 'the kind of sample being processed: test or control'
    branch.sample=branch.name
    from(sample + '.' + type + '.bam'){
        exec "java -jar /opt/nesi/mahuika/picard/2.1.0/picard.jar CollectInsertSizeMetrics I=$input O=$output.sampleMetrics.txt H=$output.sampleHist.pdf M=0.5","collectMetrics"
    }
}

alignmentMetrics = {
    output.dir=$intermediateDirectory
    requires type: 'the kind of sample being processed: test or control'
    branch.sample=branch.name
    println "$sample"
    
    // for collecting % on target
    //regions = "/nesi/project/uoa00571/data/design_files/S07604514_Regions.bed"
    regions = "/nesi/project/uoa02461/data/xgen-exome-research-panel-targetsae255a1532796e2eaa53ff00001c1b3c.bed"

    from(sample + '.' + type + '.bam'){
        exec """ samtools index -@ $threads $input > $output.indexed""" , "index"
        exec """ java -Xmx8G -jar /nesi/project/uoa00571/src/cpipe-master/tools/gatk/2.3.9/GenomeAnalysisTK.jar -T DepthOfCoverage -I $input.bam -R /nesi/project/uoa02461/data/hg19/ucsc.hg19.fasta -o $output  --omitDepthOutputAtEachBase --omitIntervalStatistics --omitLocusTable --printBaseCounts -L $regions -ct 1 -ct 10 -ct 30 -ct 50 -ct 100 -ct 200 > $output """, "alignmentMetrics"
        exec """ java -Xmx8G -jar $picardLoc CollectAlignmentSummaryMetrics I=$input.bam R=$bwaIndex O=$output.AlignmentMetricsLog""" , "alignmentMetrics"
    }
}


convertToSam = {
        output.dir=$intermediateDirectory
        exec "samtools view -h -o $output.sam $input.bam", "convertToSam"
}



filterShortInserts = {
    output.dir=$intermediateDirectory
    //exec "awk -F '\t' '((\$1~/@/) || (\$9 <= -50 && \$9 >= -150) || (\$9 <= 150 && \$9 >= 50)) { print \$0 }' $input.sam > $output.sam"
    exec "awk '{ print \$1 '\t' \$2 }' "

}

convertToBam = {
    output.dir=$intermediateDirectory
    exec "samtools view -S -b input.sam > output.bam"
}


pileUp = {
    output.dir=$intermediateDirectory
    branch.sample = branch.name
    
    produce(sample + '.snp.vcf', sample + '.indel.vcf') {
        from(sample + '*removeSuplementary.bam') {
            // This is bad form, in as much as the stages are defined in the input filenames here rather than being added dynamimcally. 
            // It'd be really nice if someone could figure out how to fix that he said nonchalantly
            exec """samtools mpileup -P $threads -B -d 9001 -q 1 -f $bwaIndex  $input.control.removeDuplicates.removeSuplementary.bam $input.test.removeDuplicates.removeSuplementary.bam | java -Xmx16g -d64 -jar /nesi/project/uoa00571/bin/VarScan.v2.4.3.jar somatic --mpileup 1 --min-var-freq 0.1 --p-value 1.00 --somatic-p-value 1.00 --strand-filter 0 --tumor-purity 0.5 --output-vcf 1 --min-coverage-normal 10 --min-coverage-tumor 10 --output-snp $output.snp.vcf  --output-indel $output.indel.vcf""","pileUp"
        }
    }
}

annotation = {
    output.dir=$intermediateDirectory
    branch.sample = branch.name
    println "Annotating sample $sample"
    // Version 2 of our annotation
    produce(sample + ".annotated.snp.vcf", sample + ".annotated.indel.vcf"){
        from(sample + '*.snp.vcf') {
            println "$input.vcf"
            exec """perl /nesi/project/uoa00571/src/table_annovar.pl $input.vcf /nesi/project/uoa00571/data/annovar/db -buildver hg19 -outfile $output.annotated.snp.vcf -remove -protocol refGene,popfreq_max_20150413,dbnsfp30a,tfbsConsSites,targetScanS,genomicSuperDups,clinvar_20170130,cosmic82,avsnp147,rmsk,wgEncodeDacMapabilityConsensusExcludable,wgEncodeDukeMapabilityRegionsExcludable,gnomad_genome,gnomad_exome,exac03,intervar_20170202,revel,dgvMerged  -operation g,f,f,r,r,r,f,f,f,r,r,r,f,f,f,f,f,r  -nastring . -vcfinput""", "annotation"
        }
        from(sample + '*.indel.vcf'){
            println "$input"
            exec """perl /nesi/project/uoa00571/src/table_annovar.pl $input.vcf /nesi/project/uoa00571/data/annovar/db  -buildver hg19 -outfile $output.annotated.indel.vcf -remove -protocol refGene,popfreq_max_20150413,dbnsfp30a,tfbsConsSites,targetScanS,genomicSuperDups,clinvar_20170130,cosmic82,avsnp147,rmsk,wgEncodeDacMapabilityConsensusExcludable,wgEncodeDukeMapabilityRegionsExcludable,gnomad_genome,gnomad_exome,exac03,intervar_20170202,revel,dgvMerged  -operation g,f,f,r,r,r,f,f,f,r,r,r,f,f,f,f,f,r  -nastring . -vcfinput""", "annotation"

        }
    }
}

// filter out mutations found in dbsnp150
filterdbSnp150 = {
    output.dir=$intermediateDirectory
    exec "bedtools intersect -v  -a $input.vcf -b $dbSnp150 > $output.vcf"
}




// filter with cosmic mutations - keep anything that is in either cosmic coding or cosmic non-coding vcf's
filterCosmic = {
    output.dir=$intermediateDirectory
    exec "bedtools intersect  -a $input.vcf -b $cosmic > $output.vcf"
}


count = {
  output.dir="/nesi/nobackup/uoa02606/data/intermediateFiles"
  zipped = "$input1" + ".gz"
  exec "bgzip -c -f $input1.vcf > $zipped"
  exec "tabix -f -p vcf $zipped"

  zipped2 = "$input2" + ".gz"
  exec "bgzip -c -f $input2.vcf > $zipped2"
  exec "tabix -f -p vcf $zipped2"


  //This really doesn't fall over gracefully when the filters are to strict.
  //Also, this is appallingly bad form. The only difference between the two calls are a bit flag and the presence/absence of some filters.
  //it should be a single function called twice with different parameters.
  exec "Rscript /nesi/project/uoa00571/src/reportGeneration/filePairs.R $zipped $zipped2 $output"
  exec "Rscript /nesi/project/uoa00571/src/reportGeneration/filePairsSomatic.R $zipped $zipped2 $output2"
   //exec "Rscript src/filePairs.R $input1.vcf.gz $input2.vcf.gz $output"
   //exec "Rscript src/filePairsSomatic.R $input1.vcf.gz $input2.vcf.gz $output2"


}


reportGeneration ={
   output.dir="../results"
   exec "Rscript /nesi/project/uoa00571/src/reportGeneration/generateReport.R $input1 $input2 $output1"
}

// currently runs qc, doesn't wait for confirmation on whether to continue the run, 
// trims, and does a 2nd lot of qc to the trimmed reads. 
// Once this is parrallelized, should be able to insert multiqc steps. 

// run with qc
//run { [qc + trim] +  [qc + align] + removeDuplicates + removeSuppplementary + pileUp + variantCalling + annotation}

// actual plasma run ...
//run { trim +  align + markDuplicates + removeSuppplementary +  convertToSam + filterShortInserts + convertToBam + pileUp + variantCalling + annotation + filterdbSnp150 + filterCosmic}
//run {
//   "%_*.fastq.gz" * [trim] + "%_*.trim" * [align] + "%_*" * [removeDuplicates] + "%_*" * [removeSuppplementary] + branches * [pileUp] +  "%_*" * [annotation]
//}

//run { "%_*.fastq.gz" * [trim]  + "%_*.trim" * [align] + "%_*" * [removeDuplicates, alignmentMetrics] }

